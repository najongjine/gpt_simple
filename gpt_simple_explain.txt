graph TD
    %% 스타일 정의
    style Input fill:#f9f,stroke:#333,stroke-width:2px,color:black
    style Embedding fill:#bbf,stroke:#333,stroke-width:2px,color:black
    style Block fill:#dfd,stroke:#333,stroke-width:2px,color:black
    style Attention fill:#fff,stroke:#333,stroke-dasharray: 5 5,color:black
    style Output fill:#fdd,stroke:#333,stroke-width:2px,color:black

    %% 1. 입력 단계
    Input(1. 입력: 'Hello') --> Tokenizer["토크나이저 (tiktoken)<br/>문자를 숫자로 변환"]
    Tokenizer --> IDs["정수 인덱스 [15496, ...]"]

    %% 2. 임베딩 단계
    IDs --> Emb["2. 임베딩 (Embedding)<br/>숫자를 의미있는 벡터(좌표)로 변환"]
    Emb --> Pos["+ 포지셔널 임베딩<br/>(단어의 위치 정보 추가)"]
    
    %% 3. 트랜스포머 블록 (반복)
    Pos --> B_Start("3. 트랜스포머 블록 (Block) 진입")
    
    subgraph "Block (생각하는 단계)"
        direction TB
        B_Start --> LN1[LayerNorm]
        LN1 --> Att["Multi-Head Attention<br/>(문맥 파악: 과거 정보 취합)"]
        Att --> Add1((+))
        B_Start --> Add1
        
        Add1 --> LN2[LayerNorm]
        LN2 --> FF["FeedForward (MLP)<br/>(정보 섞기 & 추론)"]
        FF --> Add2((+))
        Add1 --> Add2
    end
    
    %% 4. 출력 단계
    Add2 --> LN_Final[최종 LayerNorm]
    LN_Final --> Head["4. 출력 헤드 (Linear)<br/>단어장 크기로 확장"]
    Head --> Softmax["Softmax (확률 계산)"]
    Softmax --> Predict("다음 단어 예측: 'World'")

    %% 루프
    Predict -- "예측한 단어를 뒤에 붙여서 다시 입력" --> Input




nn.Linear(n_embd, 4 * n_embd)

숫자를 늘린다음 relu 처리하고 다시 쭐인다고??

뭔 발상이야 이건
--->
맞아요, 직관적으로 보면 "기껏 늘렸다가 왜 다시 줄여? 그냥 처음부터 그 크기로 하든가, 낭비 아니야?" 라는 생각이 드는 게 당연합니다.

하지만 이 "뻥튀기 후 압축" 과정이 사실 딥러닝에서 지능이 생기는 핵심 마법입니다. 왜 그런지 **"꼬인 이어폰 줄"**로 설명해 드릴게요.

1. 꼬인 줄을 풀려면 '공간'이 필요하다
데이터가 n_embd(예: 64)라는 좁은 공간에 있을 때는, 정보들이 서로 뒤엉켜 있습니다. 마치 주머니 속에서 꽉 뭉쳐진 이어폰 줄 같습니다.

n_embd 상태: 좁은 책상 위에서 꼬인 줄을 풀려고 하면, 공간이 부족해서 이리저리 치이다가 더 꼬입니다.

4 * n_embd (뻥튀기): 이걸 **넓은 거실 바닥(4배 넓은 공간)**으로 확 던져서 펼칩니다.

ReLU (작업): 공간이 넓어지니 줄이 어디가 꼬였는지 눈에 확 들어옵니다. 이제 가위질을 하든 펴든, **"작업"**을 하기가 훨씬 쉬워집니다.

다시 n_embd (압축): 잘 정리된 줄을 다시 예쁘게 말아서 주머니에 넣습니다.

즉, "복잡한 문제를 넓은 곳에서 쉽게 풀기 위해" 차원을 뻥튀기하는 것입니다.

2. 수학적인 이유: 차원이 높을수록 나누기 쉽다
이걸 좀 더 있어 보이는 말로 "선형 분리 가능성(Linear Separability)" 문제라고 합니다.

2차원(종이)에서: 빨간 점과 파란 점이 막 섞여 있으면, 직선 하나만 그어서 완벽하게 가를 수가 없습니다.

3차원(공간)으로 들어 올리면: 점들을 위아래로 쭉 당겨서 입체적으로 만들면, 판 하나를 그 사이에 끼워 넣어서 깔끔하게 분리할 수 있습니다.

FeedForward가 하는 일이 바로 이겁니다.

Linear(4배): 2차원 종이 위의 점들을 3차원 공중으로 붕 띄워서 펼쳐줍니다. (복잡한 패턴을 분해)

ReLU: 공중에 뜬 점들 사이로 불필요한 부분(음수)을 싹둑 잘라냅니다. (비선형성 추가)

Linear(원상복구): 잘 정리된 정보를 다시 종이 위에 깔끔하게 내려놓습니다.

요약
"좁은 곳에서는 해결 안 되는 복잡한 문제(데이터)를, 넓은 차원으로 데려가서 널찍하게 펼쳐놓고 해결한 뒤 다시 가져오는 과정입니다."
